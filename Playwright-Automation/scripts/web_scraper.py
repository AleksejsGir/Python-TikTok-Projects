# -*- coding: utf-8 -*-

"""
Веб-скрапер для сбора данных с сайта Python.org с использованием Playwright.

Этот скрипт демонстрирует:
- Сбор данных с веб-страницы (парсинг контента)
- Обработку собранных данных (извлечение нужной информации)
- Сохранение данных в файл CSV (для дальнейшего анализа)

Веб-скрапинг (web scraping) - это процесс автоматического сбора информации с веб-сайтов.
Эта техника позволяет извлекать нужные данные с сайтов, которые не предоставляют API
или возможность экспорта данных.
"""

# Импортируем необходимые библиотеки
from playwright.sync_api import sync_playwright  # Для автоматизации браузера
import csv  # Для работы с CSV-файлами
import os  # Для работы с файловой системой


def scrape_python_events(playwright):
    """
    Функция для сбора информации о событиях с сайта Python.org.

    Эта функция:
    1. Открывает страницу с событиями Python
    2. Находит все события на странице
    3. Извлекает детали каждого события (название, место, время, URL)
    4. Сохраняет собранные данные в CSV-файл

    Параметры:
    playwright -- объект Playwright, предоставляющий доступ к браузерам
    """
    # Запускаем браузер
    # headless=True по умолчанию (браузер без графического интерфейса)
    # Это лучший режим для скрапинга, так как он быстрее и потребляет меньше ресурсов
    browser = playwright.chromium.launch()

    # Создаем новую страницу
    page = browser.new_page()

    # ---- Шаг 1: Загрузка страницы с данными ----

    # Переходим на страницу событий Python.org
    # Эта страница содержит список предстоящих событий, связанных с Python
    page.goto("https://www.python.org/events/python-events/")

    # ---- Шаг 2: Поиск элементов с данными ----

    # Находим все события на странице с помощью CSS-селектора
    # ".list-recent-events li.event-item" означает "найти все элементы li с классом event-item
    # внутри элементов с классом list-recent-events"
    events = page.locator(".list-recent-events li.event-item")

    # Подсчитываем количество найденных событий
    count = events.count()

    print(f"Найдено {count} событий")

    # ---- Шаг 3: Подготовка структуры данных ----

    # Подготавливаем данные для CSV
    # Первая строка содержит заголовки столбцов
    csv_data = [["Название", "Место", "Время", "URL"]]

    # ---- Шаг 4: Извлечение данных из каждого элемента ----

    # Проходим по каждому событию и извлекаем информацию
    for i in range(count):
        # Получаем i-й элемент из коллекции событий
        # nth(i) выбирает элемент с индексом i (начиная с 0)
        event = events.nth(i)

        # Извлекаем детали события, используя CSS-селекторы для поиска нужных элементов внутри события

        # Название события (текст внутри ссылки в элементе с классом event-title)
        title = event.locator(".event-title a").inner_text()

        # Место проведения (текст в элементе с классом event-location)
        location = event.locator(".event-location").inner_text()

        # Время проведения (текст в теге time)
        time = event.locator("time").inner_text()

        # URL события (значение атрибута href у ссылки в элементе с классом event-title)
        url = event.locator(".event-title a").get_attribute("href")

        # Если URL относительный (начинается с /), делаем его абсолютным
        # Это нужно, чтобы ссылки в CSV-файле были полными и работали при клике
        if url and url.startswith("/"):
            url = f"https://www.python.org{url}"

        # Добавляем данные в список
        # Каждая строка в csv_data будет соответствовать одной строке в CSV-файле
        csv_data.append([title, location, time, url])

        print(f"Событие {i + 1}: {title}")

    # ---- Шаг 5: Создание директории для сохранения данных ----

    # Создаем директорию для данных, если её нет
    # "../data/" означает "папка data в родительской директории"
    os.makedirs("../data", exist_ok=True)

    # ---- Шаг 6: Сохранение данных в CSV-файл ----

    # Открываем файл для записи
    # "w" - режим записи (файл будет создан или перезаписан)
    # newline="" - важный параметр для корректной работы с CSV в Windows
    # encoding="utf-8" - кодировка файла (поддерживает Unicode, включая русские буквы)
    with open("../data/python_events.csv", "w", newline="", encoding="utf-8") as f:
        # Создаем объект для записи CSV
        writer = csv.writer(f)

        # Записываем все строки из нашего списка данных
        # writerows() записывает несколько строк сразу
        writer.writerows(csv_data)

    print(f"Данные сохранены в файл ../data/python_events.csv")

    # ---- Шаг 7: Создание скриншота для визуальной проверки ----

    # Делаем скриншот страницы с событиями
    # Это полезно для визуальной проверки данных, которые мы собрали
    page.screenshot(path="../screenshots/python_events.png")

    # Закрываем браузер и освобождаем ресурсы
    browser.close()


# Запускаем наш скрапер с Playwright
with sync_playwright() as playwright:
    scrape_python_events(playwright)

# Советы по веб-скрапингу:
#
# 1. Этические и правовые аспекты:
#    - Перед скрапингом ознакомьтесь с robots.txt сайта (например, https://www.python.org/robots.txt)
#    - Не нагружайте сайт множеством запросов за короткое время (добавляйте задержки)
#    - Проверьте Terms of Service сайта - некоторые явно запрещают скрапинг
#    - Используйте собранные данные только в соответствии с законом (например, не для спама)
#
# 2. Технические советы:
#    - Для больших объемов данных используйте пагинацию (переход по страницам)
#    - Сохраняйте промежуточные результаты, чтобы не потерять данные при ошибке
#    - Для повторяющихся задач настройте автоматический запуск по расписанию
#    - Используйте try/except для обработки ошибок при скрапинге
#
# 3. Форматы данных:
#    - CSV подходит для простых табличных данных (как в этом примере)
#    - JSON лучше для иерархических или сложных данных
#    - SQLite или другие базы данных подходят для больших объемов данных с частыми запросами
#
# 4. Альтернативы:
#    - Проверьте, предоставляет ли сайт API (это предпочтительнее скрапинга)
#    - Для некоторых популярных сайтов существуют готовые библиотеки-парсеры
#    - RSS-ленты могут содержать нужные данные в структурированном виде